---
title: Computationally Efficient Features for Leaf Image Classification
authors:
  - name: Jayani P.G. Lakshika
    thanks: Use footnote for providing further information about author (webpage, alternative address)---*not* for acknowledging funding agencies. Optional.
    department: Department of Statistics
    affiliation: University of Sri Jayewardenepura
    location: Nugegoda, Sri Lanka
    email: jayanilakshika76@gmail.com
  - name: Thiyanga S. Talagala
    department: Department of Statistics
    affiliation: University of Sri Jayewardenepura
    location: Nugegoda, Sri Lanka
    email: ttalagala@sjp.ac.lk
abstract: |
  Enter the text of your abstract here.
keywords:
  - scagnostics
  - image processing
  - bloo
  - these are optional and can be removed
bibliography: references.bib
biblio-style: unsrt
output: 
  rticles::arxiv_article:
    keep_tex: true
longtable: true
header-includes:
  - \usepackage{longtable}
  - \usepackage{amsmath, xparse}
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.pos = "!ht")
```

```{r,echo=FALSE, comment=NA, message=FALSE, warning=FALSE}
library(here)
library(knitr)
library(tidyverse)
library(patchwork)
```

# Introduction

|       Leaf identification is becoming very popular in classifying plant species. Leaf contains significant features that can help people to identify and classify the plant species by looking at it. In medical perspective, medicinal plants are usually identified by practitioners based on years of experience through sensory or olfactory senses. The other method of recognizing these plants involves laboratory-based testing, which requires trained skills, data interpretation which is costly and time-intensive. Automatic ways to identify medicinal plants are useful especially those that are lacking experience in medicinal plant recognition. In this process, image processing, and feature extraction have more weight, because they are the initial step in identification. 

|       Image processing is a crucial step in plant classification. The main aim of image processing is to improve the leaf image by removing undesired distortion [@articlee]. Therefore processed leaf image can be used to extract features. Image segmentation [@DBLP], image orientation, cropping, grey scaling, binary thresholding, noise removal, contrast stretching, threshold inversion, image normalization [@article10], and edge recognition [@articlepl;@8675114;@article5] are some of image processing techniques applied in recent research. Image processing steps can be applied parallel or individually or several times until the quality of leaf image reaches satisfactory level. 

|       Most challenging part is to extract distinctive leaf features. Therefore most of the time research more focused on neural network models like CNN [@4458016;@articlepl;@inproceedings] which are complicated and hard to understand what happening inside the algorithm. Feature extraction refers to taking measurements, geometric or otherwise, of possibly segmented, meaningful regions in the image [@articlee]. A digital image is merely a collection of pixels represented as large matrices of integers corresponding to intensities of colors at different positions of the image [@book1]. The main aim of feature extraction is to reduce dimensionality of this information by obtaining patterns of leaf images. In generally shape, colors, and texture contain these patterns. In this paper other than existing features, we are introducing scagnostic features, correlation of cartesian coordinate, and number of minimum and maximum points as new features.  

|       The paper is organized as follows. In section 2, we briefly review the works on features of leaves that use in leaf classification, and image processing techniques that have to follow before extract the features. The next section, 3, discuss the datasets that we use to do the experiment. Steps of image processing of leaf images describes in section 4. In section 4, discuss about feature extraction in in-detail because features are highly influenced by the plant species to be classified. The next section, 5, show the experimental results for two existing datasets. The details about software and packages that use to extract the features are discussed in section 6. In section 7, discuss about visualization of leaf images in the feature space. Some discussion about the outputs and concluding remarks are given in last section.       

# Literature Review




#  Data

|       There are two leaf datasets of two different countries are used for the experiment, because to generalize the features we have to have leaf images collected by different countries in different methods like scans, pseudo-scans and photography.

## Flavia Leaf Image Dataset
    
|       The Flavia dataset contains 1907 leaf images. There are 32 different species and each have 50-77 images. Scanners and digital cameras are used to acquire the leaf images on plain background. The isolated leaf images contain blades only, without petiole. These leaf images are collected from the most common plants in Yangtze, Delta, China [@articlee]. Those leaves were sampled on the campus of the Nanjing University and the Sun Yat-Sen arboretum, Nanking, China [@articlee]. (\url{https://sourceforge.net/projects/flavia/files/Leaf%2520Image%2520Dataset/})



```{r, echo=FALSE, out.width="50%",fig.align='center',fig.cap="\\label{slp1}Sample of Flavia dataset"}
knitr::include_graphics(here::here("leaffeatures","Figures","flavia_images.png"))
```



## Swedish Leaf Image Dataset
    
|       The Swedish dataset contains 1125 images. The images of isolated leaf scans on a plain background of 15 Swedish tree species, with 75 leaves per species. This dataset has been captured as part of a joined leaf classification project between the Linkoping University and the Swedish  Museum of Natural History [@articlee]. (\url{https://www.cvl.isy.liu.se/en/research/datasets/swedish-leaf/})



```{r, echo=FALSE, out.width="50%", fig.align='center',fig.cap="\\label{slp2}Sample of Swedish leaf dataset"}
knitr::include_graphics(here::here("leaffeatures","Figures","swedish_data.png"))
```



# Image Processing

## Introduction

|       The image processing receives an image as input and generates a modified image as an output which is suitable for better morphological analysis [@8675114], feature extraction. Image processing is an essential step to reduce noise, background subtraction and content enhancement in the identification process [@8675114].


## Image Processing Workflow

|       In identifying a plant species, leaves are considered as a dominant feature. Image preprocessing plays a vital role, because features have to be clearly found after preprocessing. The main idea of preprocessing is to remove any kind of external noises present in an image.
|       To improve the leaf image, a series of operations are followed. This research includes operations like converting BGR image to RGB, gray scaling, Gaussian filtering, binary thresholding, remove stalk, close holes, and image resizing. Some of these steps can be applied for the necessary cases like applying remove stalk to the leaf images which only have a stalk.


\begin{figure}[!ht]
\centering
		\includegraphics[scale=0.65]{./Figures/fl5.png}
		\caption{\label{fig:test2}Image processing}
	    \end{figure}


|       For the development of the system proposed the focus was on the leaf which has a simple arrangement.

\begin{figure}[!ht]
\centering
		\includegraphics[scale=0.5]{./Figures/simple_leaf_parts.png}
		\caption{\label{simplearra}Leaf with simple arrangement}
	    \end{figure}



\begin{table}[h!]
\centering
\begin{tabular}{l c c }

\hline
Dataset                   & \multicolumn{1}{l}{Original Image Step} & \multicolumn{1}{l}{Number of leaf images}  \\ \hline

Flavia dataset            & Step 1 & 1907                                                                      \\ %\hline

Swedish Leaf Image Dataset      & Step 1 & 975                                                                       \\                                                                    \hline

\end{tabular}
\caption{Number of leaf images used in the algorithm}
\label{tab:num}
\end{table}


### Step 1: Converting BGR Image to RGB

|       BGR (Blue-Green-Red) and RGB(Red-Green-Blue) are conventions for the order of the different colour channels. They are not colour spaces. When converting BGR image to RGB, there is no any computations, just switches around the order. There is a difference in OpenCV and Matplotlib in pixel ordering. OpenCV follows BGR order while Matplotlib follows RGB order. Therefore when we want to display an image which loaded from OpenCV using Matplotlib functions, have to convert it to RGB mode.

### Step 2: Grayscaling

|       Grayscaling is the process of converting an image to shades of gray from other colour spaces like RGB. Gray conversion of the image is implemented to optimize the contrast and intensity of images [@8675114]. There are some importance of grayscaling:

* Dimension reduction

|       As an example, there are three colour channels in RGB images and has three dimensions. But in gray scaled images only have one dimension.
    
* For other algorithms to work

|       There are many algorithm customized to work only on gray scaled images. Eg:  Haralick texture features calculation works only on gray scaled images.
    
* Reduced model complexity

|       Consider an example of training neural article on RGB images of 10x10x3 pixel. The input layer will have 300 input nodes. Whereas for gray scaled images the same neural network will need only 100 input node.


### Step 3: Image Smoothing

|       Image smoothing is also known as image blurring in OpenCV. Image smoothing refers to making the image less clear or distinct. Image smoothing is done with the help of various low pass filter kernels. 

\textbf{Advantages of Smoothing}

\begin{itemize}
    \item Smoothing techniques help in noise removal. Noise of an image is considered as high pass signal which can be occurred because of the source (camera sensor). Therefore restrict noise by using the low pass kernel.
    \item Helps in smoothing image.
    \item To remove low intensity edges.
    \item Helps in hiding the details when necessary. E:g:- To hide the face of the victim in police cases.
\end{itemize}

|       There are many image smoothing techniques that are available in OpenCV. For our research we used Gaussian filtering as the image smoothing technique.

### Step 4: Gaussian Filtering (Gaussian Blurring/Gaussian Smoothing)

|       Gaussian function is used to blur the image. It is a linear filter which is done by using the functions in OpenCV. By specifying the width and height of the Gaussian kernel that must be positive and odd, and specifying the kernel standard deviation along x and y-axis, Gaussian smoothing is established in OpenCV. When the kernel standard deviation along x-axis is specified, kernel standard deviation along y-axis is taken as equal to the the kernel standard deviation along x-axis. But if both kernel standard deviation are given as zeros, they are calculated by using the kernel size. In our research the width and height of the kernel is defined as 55 and the kernel standard deviation along x-axis is assigned as zero.


### Step 5: Binary Thresholding

|       Thresholding is a segmentation technique that is used to separate foreground from its background. In thresholding technique, the pixel values are assigned by using the threshold value. By comparing each pixel value with the threshold value, the thresholding technique is worked.If the pixel value is smaller than the threshold value, the pixel value is set as 0, and if not the pixel value is set to a maximum value which is generally 255. Thersholding technique is done on grayscale images in Computer Vision. 

|       We used Otsu's binarization which is an adaptive thresholding after Gaussian filtering to convert colour images to the binary images. In Otsu's method, the threshold value is determined automatically. The algorithm of Otsu's method finds the optimal threshold value which is chosen arbitrary.

\textbf{Otsu's Thresholding}

|       Nobuyuki Otsu is the investor of Otsu's method which is defined for a gray scale histogram $h_I$ of an input image $I$. To segment an image $I$ into two subsets of pixels Otsu's method calculates an optimal threshold $\tau$. Image $I$ is defined on a regular carrier $\Omega$ containing $|\Omega|$ pixel locations. 

|       The algorithm maximizes the variance $\sigma^2$ between the two subsets (Within-class-variance) to find the threshold $\tau$.

\[\sigma^2 = P_1(\mu_1-\mu)^2 + P_2(\mu_2-\mu)^2 = P_1P_2(\mu_1-\mu_2)^2 \]

where $\mu$ is the mean of the histogram,
$\mu_1$ and $\mu_2$ are the mean values of first and second subset respectively,
$P_1$ and $P_2$ are the corresponding probabilities of the two clusters, defined by

\[P_1 = \frac{\sum_{i=0}^{u}h_I(i)}{|\Omega|}\]

\[P_2 = \frac{\sum_{i=u+1}^{255}h_I(i)}{|\Omega|}\]

Where u is the candidate threshold and the maximum gray level ($G_{max}$) is assumed as 255. To find optimal threshold $\tau$ for segmenting image $I$, all candidate thresholds are evaluated this way.

The algorithm of Otsu's method is defined as follows,

\begin{table}[!ht]
\centering
\begin{tabular}{l}
\hline
Compute the histogram of the grayscale image \\ 
Set the histogram variance $S_{max} = 0$                                             \\ \hline
\textbf{while} $u < G_{max}$ \textbf{do}                                             \\ \hline
Compute $\sigma^2 = P_1P_2(\mu_1-\mu_2)^2$                                             \\ 
\textbf{if} $\sigma^2 > S_{max}$ \textbf{then}                                             \\ \hline
$S_{max} = \sigma^2$                                             \\
$\tau = u$                                             \\ \hline
\textbf{end if}                                             \\ 
Set $u = u+1$                                             \\ \hline
\textbf{end while}                                             \\ \hline
\end{tabular}
\caption{Otsu's method}
\label{tab:ot}
\end{table}


### Step 6: Image Resizing

|       There are two different leaf image datasets (Flavia, Swedish) which have different sizes. To compare the results on different datasets, to improve the memory storage capacity and to reduce computational complexity the leaf images are resized to a fixed resolution. In our study, the leaf images have been resized to [1600 x 1200px] which is the size of Flavia leaf images. 


|       Other than the main image processing techniques, the following two techniques are applied in some or all cases as an image processing techniques after image thresholding.


*) \textbf{Remove Stalk}

|       Remove the petiole (stalk) of leaf image is another version of thresholding process. Thresholding is applied after finding the sure foreground area. To find the sure foreground area, distance transform technique is used.  Binary image is used as the input of distance transform technique. In distance transform technique, image is created by assigning a number for each object pixel that corresponds to the distance to the nearest background pixel. The distance is calculated using the euclidean distance (\ref{eu}). After finding the sure foreground area, Otsu's binarization is applied again as the thresholding technique.

\begin{equation}
    Euclidean distance = \sqrt{\sum_{i=1}^{n}(q_i - p_i)^2}
    \label{eu}
\end{equation}

where $n$ = n-space

$q_i,p_i$ = Euclidean vectors, starting from the origin of the space

```{r, echo=FALSE, out.width="50%", fig.align='center',fig.cap="\\label{fig:rst}Remove stalk"}
knitr::include_graphics(here::here("leaffeatures","Figures","remove_stalk.png"))
```


*) \textbf{Closing Holes}

|       Closing holes is followed by noise removal technique which demonstrates the closing effect. Closing effect is used to remove small holes inside the foreground objects. Closing holes is a result of morphological transformation. morphological transformation are the operations based on image shape. Binary image is used as one input in morphological transformation. Kernel or structuring element which decides the nature of the operation is used as the second input. There are two morphological operators as Erosion and Dilation. Closing is a variant form of morphological operators which is used in closing holes process. Closing is also know as Dilation followed by Erosion.

\textbf{Erosion}

|       The basic idea of Erosion is that erodes away the boundaries of foreground object. Since the input is binary image, a pixels in the original image is either 1 or 0. If all the pixels under the kernel is 1, a pixel of original image is considered as 1, otherwise made to zero (eroded). Which means that depending upon the size of the kernel all pixels near boundary will be discarded. Therefore the thickness or size of the foreground object decreases (White region of the image decreases). 


\textbf{Dilation}

|       Opposite of erosion is defined as Dilation. If at least one pixel under the kernel is 1, the pixel element is 1 in Dilation. It tends to increase the foreground of the image or the white region  of the object. 

|       Noise removal is that a technique of erosion is followed by Dilation. In erosion white noise is removed and shrinks the object (dilate) which doesn't come back. But in closing holes approach the white area is increased. 


```{r, echo=FALSE, out.width="50%", fig.align='center',fig.cap="\\label{fig:chl}Closing holes"}
knitr::include_graphics(here::here("leaffeatures","Figures","close_holes.png"))
```

# Features

|       In identification of plant species by using leaf images, features of the leaves play a main role, because each leaf posses unique feature that it make different from other. In previous research [@articlepl;@article7], let the algorithm like CNN to extract features by itself and do the classification. Therefore it is so hard to interpret and generalize the features. We introduced pre-calculate features which can be easy to interpret and generalize. They are also computational efficient. Mainly we focused on four types of features of leaf images as Shape features, Texture features, Color features and Scagnostics features. We identified altogether 52 features.


```{r, echo=FALSE, out.width="70%", fig.align='center',fig.cap="\\label{img33}Example of geometric transformation"}
knitr::include_graphics(here::here("leaffeatures","Figures","feature_hie.png"))
```




## Shape Features

|       When identifying real-world objects, the shape is known as an essential sign for humans. A shape measure is a quantity, which relates to a particular shape characteristic of an object in general [@articlee]. 
	
|       The main geometric transformation are rotation reflection, scaling and translation (see figure \ref{img3}). In my research, the defined shape descriptors (shape features) are invariant to the rotation and reflection. But some limitations are applied in translation and scaling. 
	
	
```{r, echo=FALSE, out.width="70%", fig.align='center',fig.cap="\\label{img3}Example of geometric transformation"}
knitr::include_graphics(here::here("leaffeatures","Figures","geomatric_transformation.png"))
```
	
	
|       The finding contour function doesn’t work when the center of the leaf image is not in the contour. Therefore if the translation is applied away from the contour \ref{trans}, then function can’t identify the contour. Inappropriate scaling (see figure \ref{scal}) also arises problems in the calculation. If the leaf image is really small, the function also hard to recognize the contour. Therefore taking the closest photo of the leaf images by keeping them in the center of the white paper is more suitable.

```{r, echo=FALSE, out.width="10%", fig.align='center',fig.cap="\\label{trans}Inappropriate translation"}
knitr::include_graphics(here::here("leaffeatures","Figures","trans.jpg"))
``` 


```{r, echo=FALSE, out.width="10%", fig.align='center',fig.cap="\\label{scal}Inappropriate scaling"}
knitr::include_graphics(here::here("leaffeatures","Figures","scaling.jpg"))
``` 

## Contours

Simply contour (see figure \ref{cnt}) is a curve joining all the continuous points (along the boundary), having the same colour or intensity.

```{r, echo=FALSE, out.width="50%", fig.align='center',fig.cap="\\label{cnt}Extract contour of the leaf image"}
knitr::include_graphics(here::here("leaffeatures","Figures","cnt.png"))
```  


|       Shape descriptors can be classified into two main categories as contour-based and region-based. Contour-based descriptors extract shape features solely from the contour of a shape [@articlee]. Whereas, region-based descriptors obtain shape features from the whole region of a shape [@articlee]. In addition, there also exist some methods, which cannot be classified as either contour-based or region-based (see figure \ref{scalimg4}).


```{r, echo=FALSE, out.width="30%", fig.align='center',fig.cap="\\label{scalimg4}Categorization of shape features"}
knitr::include_graphics(here::here("leaffeatures","Figures","shape_chart.png"))
```  



|       Through this research, we restricted our research to the following shape features to identify leaf images (see \ref{tab:table1}).

## Diameter

|       Diameter is defined as the longest distance between any two points on the margin of the leaf [@articlee].

|       To calculate the diameter of the leaf image, firstly we have to find the contour of the leaf image. Then we have to select all pair of contour points and measure the Euclidean distance (\ref{equa_shape}) between the two points separately. Finally have to find the maximum distance among the calculated distances. (see figure \ref{shape1}) 

\begin{equation}
    d\left( A,B\right)   = \sqrt {\sum _{i=1}^{n}  \left( q_{i}-p_{i}\right)^2 }
\label{equa_shape}
\end{equation}

```{r, echo=FALSE, out.width="30%", fig.align='center',fig.cap="\\label{shape1}Logic behind calculation of diameter"}
knitr::include_graphics(here::here("leaffeatures","Figures","d_cal.png"))
```  



## Physiological length and Physiological width

|       There are horizontal, vertical and rotated leaf images in the datasets (Flavia, Swedish, Actual and Kaggle). Kaggle leaf image dataset is only consist of horizontal and vertical leaf images. Therefore straight bounding rectangle is enough to extract Physiological length and Physiological width of leaf images.

```{r, echo=FALSE, out.width="30%", fig.align='center',fig.cap="\\label{act3}Straight(Horizontal or vertical) and rotated leaf image in Actual leaf image dataset"}
knitr::include_graphics(here::here("leaffeatures","Figures","act3.png"))
```  


|       But straight bounding rectangle of rotated image doesn't give the correct values for Physiological length and Physiological width of leaf images.

```{r, echo=FALSE, out.width="30%", fig.align='center',fig.cap="\\label{act1}Bounded rectangle of rotated leaf image in Actual leaf image dataset"}
knitr::include_graphics(here::here("leaffeatures","Figures","act1.png"))
```  



|       To solve this problem, we considered rotated rectangle rather than bounded rectangle in computing shape features of angled images.

```{r, echo=FALSE, out.width="50%", fig.align='center',fig.cap="\\label{act2}Rotated rectangle of angled leaf image in Actual leaf image dataset"}
knitr::include_graphics(here::here("leaffeatures","Figures","act2.png"))
```  


|       There are two types of bounding rectangles.

\begin{itemize}
    \item Straight Bounding Rectangle
\end{itemize}

This is a straight rectangle which doesn't consider the rotation of the object. Therefore area of the bounding rectangle doesn't minimize.

\begin{itemize}
    \item Rotated Rectangle 
\end{itemize}

This bounding rectangle is drawn with minimum area. Therefore the rotation of the object is also considered. 


## Area


```{r, echo=FALSE, out.width="30%", fig.align='center',fig.cap="\\label{shape1}Area"}
knitr::include_graphics(here::here("leaffeatures","Figures","c8.png"))
```  


## Roundness/ Circularity

```{r, echo=FALSE, out.width="30%", fig.align='center',fig.cap="\\label{shape3}Circularity"}
knitr::include_graphics(here::here("leaffeatures","Figures","c6.png"))
```  

## Compactness

```{r, echo=FALSE, out.width="30%", fig.align='center',fig.cap="\\label{shape4}Compactness"}
knitr::include_graphics(here::here("leaffeatures","Figures","c10.png"))
```  

## Eccentricity

```{r, echo=FALSE, out.width="30%", fig.align='center',fig.cap="\\label{shape5}Eccentricity"}
knitr::include_graphics(here::here("leaffeatures","Figures","c9.png"))
```  

```{r, echo=FALSE, out.width="30%", fig.align='center',fig.cap="\\label{shape6}Ellipse"}
knitr::include_graphics(here::here("leaffeatures","Figures","c11.png"))
```  

\begin{equation}
    Eccentricity = \sqrt{1-\frac{b^2}{a^2}}
\end{equation}

## Convexity

```{r, echo=FALSE, out.width="30%", fig.align='center',fig.cap="\\label{shape7}Convexity"}
knitr::include_graphics(here::here("leaffeatures","Figures","c7.png"))
```  



## Convex hull

The more details about convex hull can be found in the convex hull under scagnostics features.

## Number of Minimum and Maximum Points

```{r, echo=FALSE, out.width="30%", fig.align='center',fig.cap="\\label{mn}Minimum and Maximum Points"}
knitr::include_graphics(here::here("leaffeatures","Figures","mn.png"))
```  



\begin{center}
\begin{longtable}{p{4cm} p{6.5cm} p{3cm} p{3cm}}
\caption{Definitions of shape features}
\label{tab:table1}\\

\hline \multicolumn{1}{c}{\textbf{Shape feature}} & \multicolumn{1}{c}{\textbf{Description}} & \multicolumn{1}{c}{\textbf{Pictogram}} & \multicolumn{1}{c}{\textbf{Formula}} \\ \hline 
\endfirsthead

\multicolumn{4}{c}%
{{\bfseries \tablename\ \thetable{} -- continued from previous page}} \\
\hline \multicolumn{1}{c}{\textbf{Shape feature}} & \multicolumn{1}{c}{\textbf{Description}} & \multicolumn{1}{c}{\textbf{Pictogram}} & \multicolumn{1}{c}{\textbf{Formula}} \\ \hline 
\endhead

\hline \multicolumn{4}{r}{{Continued on next page}} \\ \hline
\endfoot

\hline \hline
\endlastfoot

Centroid                                                                                                         & \begin{tabular}[c]{@{}l@{}}Represents the coordinates \\ of the leaf's geometric center\end{tabular}                                           &   \centering\includegraphics[scale=0.5]{./Figures/centroid.png}        &                                                                                      \\ %\hline
\begin{tabular}[c]{@{}l@{}}Physiological length/ \\ Major axis length (L)\end{tabular}                           & \begin{tabular}[c]{@{}l@{}}Line segment connecting the \\ base and the tip of the leaf\end{tabular}                                            &     \centering\includegraphics[scale=0.5]{./Figures/Length_new.png}      &                                                                                      \\ %\hline
\begin{tabular}[c]{@{}l@{}}Physiological width/ \\ Minor axis length (W)\end{tabular}                            & \begin{tabular}[c]{@{}l@{}}Maximum width that is \\ perpendicular to the major axis\end{tabular}                                               &    \centering\includegraphics[scale=0.5]{./Figures/width.png}       &                                                                                      \\ %\hline
Diameter (D)                                                                                                     & \begin{tabular}[c]{@{}l@{}}Longest distance between any two \\ points on the margin of the origin\end{tabular}                                 &   \centering\includegraphics[scale=0.5]{./Figures/diameter.png}        &                                                                                      \\ %\hline
Area (A)                                                                                                         & \begin{tabular}[c]{@{}l@{}}Number of pixels in the region \\ of the leaf\end{tabular}                                                          &   \centering\includegraphics[scale=0.5]{./Figures/area.png}        &                                                                                      \\ %\hline
Perimeter (P)                                                                                                    & \begin{tabular}[c]{@{}l@{}}Summation of the distance \\ between each adjoining pair of \\ pixels around the border of the leaf\end{tabular}    &   \centering\includegraphics[scale=0.5]{./Figures/perimeter.png}        &                                                                                      \\ %\hline
Aspect ratio (AR)                                                                                                & \begin{tabular}[c]{@{}l@{}}Ratio of physiological length to \\ physiological width\end{tabular}                                                & \centering\includegraphics[scale=0.5]{./Figures/AR.png}           & \[AR = \frac{L}{W}\]                                              \\ %\hline 
\begin{tabular}[c]{@{}l@{}}Roundness/\\ Circularity (R)\end{tabular}                                                                                       & \begin{tabular}[c]{@{}l@{}}Illustrate the difference between \\ a leaf and a circle\end{tabular}                                               &   \centering\includegraphics[scale=0.5]{./Figures/roudness.png}        & \[R = \frac{4 \pi A}{P^2}\]       \\ %\hline
Compactness                                                                                                      & \begin{tabular}[c]{@{}l@{}}Ratio of the perimeter over \\ the leaf's area\end{tabular}                                                         &   \centering\includegraphics[scale=0.5]{./Figures/rect.png}        & \[C = \frac{P^2}{A}\]                            \\ %\hline
Rectangularity (N)                                                                                               & \begin{tabular}[c]{@{}l@{}}Represents how rectangle a shape is,\\ i:e: how much it fits its minimum \\ bounding rectangle\end{tabular}         &           & \[N = \frac{A}{LW}\]                                              \\ %\hline
Eccentricity (E)                                                                                                 & \begin{tabular}[c]{@{}l@{}}Ratio of the distance between foci \\ of the ellipse (f) and major \\ axis length (a)\end{tabular}                  &           & \[E = \frac{f}{a}\]                                               \\ %\hline
Narrow factor (NF)                                                                                               & \begin{tabular}[c]{@{}l@{}}Ratio of the diameter over the \\ physiological length\end{tabular}                                                 & \centering\includegraphics[scale=0.5]{./Figures/nf.png}          & \[NF = \frac{D}{L}\]                                              \\ %\hline
\begin{tabular}[c]{@{}l@{}}Perimeter ratio of \\ diameter ($P_{D}$)\end{tabular}                                 & Ratio of the perimeter to the dimeter                                                                                                          &  \centering\includegraphics[scale=0.5]{./Figures/pd.png}         & \[P_D = \frac{P}{D}\]                                           \\ %\hline
\begin{tabular}[c]{@{}l@{}}Perimeter ratio of \\ Major axis length ($P_{L}$)\end{tabular}                        & \begin{tabular}[c]{@{}l@{}}Ratio of the perimeter to the \\ physiological length\end{tabular}                                                  &  \centering\includegraphics[scale=0.5]{./Figures/pl.png}         & \[P_L = \frac{P}{L}\]                                            \\ %\hline
\begin{tabular}[c]{@{}l@{}}Perimeter ratio of \\Major axis length and \\ Minor axis \\length ($P_{LW}$)\end{tabular} & \begin{tabular}[c]{@{}l@{}}Ratio of the leaf perimeter over the \\ sum of the physiological length and \\ the physiological width\end{tabular} &  \centering\includegraphics[scale=0.5]{./Figures/plw.png}          & \[P_{LW} = \frac{P}{L + W}\]                                   \\ %\hline
Number of convex points                                                                                          & \begin{tabular}[c]{@{}l@{}}Number of points to create a \\ convex hull\end{tabular}                                                            &  \centering\includegraphics[scale=0.5]{./Figures/convex.png}         &                                                                                      \\ %\hline
Perimeter convexity ($P_{C}$)                                                                                    & \begin{tabular}[c]{@{}l@{}}Ratio of the convex perimeter to the \\ perimeter of the leaf\end{tabular}                                          & \centering\includegraphics[scale=0.5]{./Figures/p_con.png}          & \[P_C = \frac{P_{CH}}{P}\]                                    \\ %\hline
Area convexity ($A_{C1}$)                                                                                        & \begin{tabular}[c]{@{}l@{}}Normalized difference of the convex \\ hull area and the leaf's area\end{tabular}                                   & \centering\includegraphics[scale=0.5]{./Figures/a_c1.png}          & \[A_{C1} = \frac{(CH-A)}{A}\]                                  \\ %\hline
Area ratio of convexity ($A_{C2}$)                                                                               & \begin{tabular}[c]{@{}l@{}}Ratio between leaf's area and area \\ of the leaf's convex hull\end{tabular}                                        &    \centering\includegraphics[scale=0.5]{./Figures/a_c2.png}       & \[A_{C2} = \frac{A}{CH}\]                                      \\ %\hline
Equivalent diameter ($D_{E}$)                                                                                    & \begin{tabular}[c]{@{}l@{}}Diameter of a circle with the the \\ same area as the leaf's area\end{tabular}                                      &  \centering\includegraphics[scale=0.5]{./Figures/eq_d.png}         & \[D_E = \sqrt{\frac{4*A}{\pi}}\] \\

\end{longtable}
\end{center}





## Texture Features

|       Texture is the term used to describe the surface of a given object or appearance and is undoubtedly a main feature used in computer vision and pattern recognition [@articlee]. Texture can only be assessed for a group of pixels whereas colour is usually a property of a pixel. Generally, texture is associated with the feel of various materials to human touch and texture image analysis is based on visual interpretation [@articlee] of this feeling. Leaf surface is a natural texture which has random persistent patterns and do not show detectable quasi-periodic structure [@articlee]. Therefore, several authors claim fractal theory to be better suited than statistical, spectral, and structural approaches for describing these natural textures [@articlee].
	
|       The Haralick texture features [@article31;@article30] are functions of the normalized GLCM (Gray Level co-occurrence Matrix - see matrix \ref{equa1}) which is a common method to represent image texture. 
    
    
$$GLCM = \begin{bmatrix}
p(1,1) & p(1,2)  & \cdot &\cdot &\cdot & p(1,N_{g}) \\ 
p(2,1) & p(2,2)  & \cdot &\cdot &\cdot & p(2,N_{g}) \\ 
\cdot  & \cdot & \cdot  & & &\cdot\\ 
\cdot  & \cdot &  & \cdot& &\cdot\\ 
\cdot  & \cdot &  & & \cdot &\cdot\\ 
p(N_{g},1) & p(N_{g},2) & \cdot &\cdot &\cdot & p(N_{g},N_{g})
\end{bmatrix}$$    
    
    
|       The GLCM (\ref{equa1}) is square with dimension $N_{g}$, where $N_{g}$ is the number of gray levels in the image [@article31]. Element [i,j] of the matrix is generated by counting the number of times a pixel with value i is adjacent to a pixel with value j and then dividing the entire matrix by the total number of such comparisons made [@article31]. Therefore each entry is considered to be the probability (see figure \ref{img1}) that a pixel with value i will be found adjacent to a pixel of value j [@article31].  Four such matrices can be calculated, because adjacency can be defined to occur in each of four directions in a 2D (see figure \ref{img2}), square pixel image (horizontal, vertical, left and right diagonals - see equation \ref{direction}) [@article31].



```{r, echo=FALSE, out.width="50%", fig.align='center',fig.cap="\\label{direction}Four directions of adjacency as defined for calculation of the Haralick texture features"}
knitr::include_graphics(here::here("leaffeatures","Figures","GLMC_direction.png"))
```    
    
	    
	 
|       The Haralick statistics are calculated for co-occurrence matrices generated using each of these directions (see figure \ref{img1}) of adjacency [@article31]. Haralick then described 14 statistics that can be calculated from the co-occurrence matrix with the intent of describing the texture of the image. Through the research, we only used the following 4 statistics among 14 of them, because most of the researchers used these 4 statistics as texture features (see figure \ref{tabmytable}) of leaf images.



\begin{table}[!ht]
\resizebox{\textwidth}{!}{%
\begin{tabular}{cll}
\hline
\multicolumn{1}{l}{Texture feature} & Description & Formula \\ \hline
Contrast                            & \begin{tabular}[c]{@{}l@{}}Measures the scale of \\ difference in the Gray \\level Co-occurrence \\ matrices.\end{tabular}            & $\sum_{n=0}^{N_{g}-1}n^{2}\left \{ \right.\sum_{i=1}^{N_{g}}\sum_{j=1}^{N_{g}}p(i,j)\left. \right \},\left | i-j \right |=n$        \\
Correlation                         & \begin{tabular}[c]{@{}l@{}}Measures of how \\correlated a pixel is to its \\ neighbour over \\ the whole image in the \\ Gay level \\ Co-occurrences matrices.\end{tabular}            & $\frac{\sum_{i}^{}\sum_{j}^{}(ij)p(i,j)-\mu_{x}\mu _{y}}{\sigma _{x}\sigma _{y}}$        \\
Entropy                             & \begin{tabular}[c]{@{}l@{}}Measures the \\ randomness of the \\ elements of the \\ co-occurrence matrix.\end{tabular}            & $-\sum_{i}^{}\sum_{j}^{}p(i,j)log(p(i,j))$        \\
Inverse Difference Moment           & \begin{tabular}[c]{@{}l@{}}Measure of homogeneity. \\ Measures the closeness \\ of the distribution of \\ elements in the GLCM \\ to the GLCM diagonal.\end{tabular}            & $\sum_{i=1}^{N_{g}}\sum_{j=1}^{N_{g}}\frac{1}{1+(i-j)^2}p(i,j)$        \\ \hline
\end{tabular}%
}
\caption{Definitions of texture features}
\label{tabmytable}
\end{table}
	
where p(i, j) = Probability density function of gray - level pairs
	
$$\mu_{x} = \sum_{i}^{}i\sum_{j}^{}p(i,j)$$ 
	
$$\mu_{y} = \sum_{j}^{}j\sum_{i}^{}p(i,j)$$ 
	
$$\sigma_{x} = \sum_{i}^{}(i-\mu_{x})^2\sum_{j}^{}p(i,j)$$
	
$$\sigma_{y} = \sum_{j}^{}(j-\mu_{y})^2\sum_{i}^{}p(i,j)$$


```{r, echo=FALSE, out.width="50%", fig.align='center',fig.cap="\\label{img1}Computing the Haralick texture features from a 4 × 4 example image step by step"}
knitr::include_graphics(here::here("leaffeatures","Figures","GLMC1.png"))
```  

```{r, echo=FALSE, out.width="50%", fig.align='center',fig.cap="\\label{img2}Computing the Haralick texture features from a 4 × 4 example image with all direction"}
knitr::include_graphics(here::here("leaffeatures","Figures","GLMC2.png"))
```  
	
	

## Color Features

|       Colour is an important feature of images [@articlee;@inproceedings1]. Colour properties are defined within a particular colour space like red-green-blue (RGB) [@colarticle1;@articlee]. Colour properties can be extracted from images after a colour space is specified. In the field of image recognition, a number of general colour descriptors have been introduced. Colour moments [@colarticle1;@articlee] are the simple descriptor among them. Mean, standard deviation skewness and kurtosis are the comment moments. Colour moments are used for characterizing planar colour patterns, irrespective of viewpoint or illumination conditions and without the need for object contour detection [@articlee]. Colour moments are convenient for real-time applications because of its low dimension and low computational complexity. 
	
|       Some of leaf images have very similar shape like Hathawariya (\ref{leafimg}) and Iramusu (\ref{leafimg}) in our experiment. Even though shapes are similar in some leaves, there are some differences in colours of leaf images. Therefore in addition to the shape features, we extracted colour based features of leaf images as well.



```{r, echo=FALSE, out.width="50%", fig.align='center',fig.cap="\\label{leafimg}"}
knitr::include_graphics(here::here("leaffeatures","Figures","leaf_img.png"))
```



|       We used mean ($\mu$) and standard deviation ($\sigma$) of intensity values of red, green and blue channels [@inproceedings1]. Mean and standard deviation of each component are calculated as follows:

\begin{equation}
    \mu = \frac{1}{MN}\sum_{x=1}^{M}\sum_{y=1}^{N}p_{xy}
\label{equa2}
\end{equation}
    
\begin{equation}
    \sigma = \frac{1}{MN}\sqrt{\sum_{x=1}^{M}\sum_{y=1}^{N}(p_{xy}-\mu)^2}
\label{equa3}
\end{equation}

where M and N are dimensions of a leaf image and $p_{xy}$ is the intensity value of pixel at (x, y) coordinate. 

## Scagnostic features

|       Scatterplot diagnostics is a term in Tukey neologism for scagnostics [@inproceedings44; @article37;@inproceedings33;@article101]. Scagnostics are characterizations of the 2D distributions of orthogonal pairwise projections of a set of points in multidimensional Euclidean space [@inproceedings44;@article37]. Measures like density, skewness, shape, outliers, and texture are included in these characterizations.
	
|       There are two people who popular in the discussion about scagnostics. John and Paul Tukey, and Wilkinson et al. [@inproceedings44;@article37] introduce their ideas about scagnostics in two angles. In first place mid of 1980s, an exploratory graphical method called scagnostics was introduced by John and Paul Tukey. A set of measures characterizing a 2D scatterplot [@inproceedings33] was the base of this method. But John and Paul Tukey were never published their ideas.
	
|       After some years later, the details collected from the first author's recollection of Institute for Mathematics and its Applications (IMA) and some discussions with Paul Tukey, Wilkinson et al. (2005) developed nine scagnostics measures defined on planar proximity graphs [@inproceedings44;@article37]. These measures were scalable to large datasets and therefore suitable for practical applications [@inproceedings44; @article37].
	
|       Wilkinson et al. was introduced nine scagnostics measures which determined the cells of scatter plot matrix (SPLOM) [@inproceedings33]. SPLOM means that the organization of scatterplots in the layout of a covariance matrix. In here the scatterplots are of the scagnostics measures.
	
|       By characterizing a large collection of 2D scatterplots through a small number of measures like the area of the peeled convex hull (Tukey 1974) [@inproceedings44;@article37], the perimeter length of this hull, the area of closed 2D kernel density isolevel contours (Silverman 1986) [@inproceedings44;@article37], the perimeter length of these contours, and a nonlinearity measure of association based on principal curves (Hastie and Stuetzle 1989) [@inproceedings44;@article37] of the arrangement of points in these plots was proposed by Tukeys. This was a simple and powerful idea, but when implementing many details wanted to involve.
	
|       Wilkinson et al. proposed his method by including the criteria that should met by candidate scagnostics.



1. Distinguish many types of point distributions: multivariate normal, lognormal, multinomial, sparse, dense, convex, clustered, etc.

2. A small number of scagnostics characterizing these distributions.

3. Should have a common scale because want to compare them with each other.

4. Should have a comparable distribution because want to compare them to standard.

5. The intrinsic dimensionality of these scagnostics, when calculated over a large number of heterogeneous scatterplots, to be as large as possible.

6. To be efficiently computable because the scagnostics should be scalable to large numbers of points and dimensions


```{r, echo=FALSE, out.width="70%", fig.align='center',fig.cap="\\label{scagimg}Hierarchy of Scagnostics"}
knitr::include_graphics(here::here("leaffeatures","Figures","scag.png"))
```


Scagnostic measures are based on following definitions.
	    
### Geometric Graphs

\begin{itemize}
    \item Graph
\end{itemize}

|       A graph G = (V, E) is defined as a set V (called vertices) together with a relation on V induced by a set E (called edges). A pair of vertices is defined as an edge $e(\nu,\omega)$, with e $\in$ E and $\nu,\omega \in$ V.

\begin{itemize}
    \item Geometric Graph
\end{itemize} 


|       An embedding of a graph in a metric space S that maps vertices to points and edges to straight line segments connecting pairs of points is defined as a geometric graph G* = [f(V), g(E), S].

|       From several features of 2D Euclidean geometric graphs, Scagnostic measures are derived.

|       The Euclidean distance between vertices that connected to edge is defined as the length of an edge, length(e).

|       The sum of the lengths of edges in graph is known as the length of a graph, length(G).

|       A list of successively adjacent, distinct edges are known as a path. If first and last vertex are the same of the path, then the path is closed.

|       A region bounded by a closed path is known as a polygon (P). A polygon bounded by exactly one closed path that has no intersecting edges is known as a simple polygon.

|       The length of boundary of a simple polygon is known as the perimeter of a simple polygon. The area of interior of a simple polygon is known as the area of a simple polygon.


```{r, echo=FALSE, out.width="50%", fig.align='center',fig.cap="\\label{scagimg5}Graph with 5 vertices and 5 edges"}
knitr::include_graphics(here::here("leaffeatures","Figures","c4.png"))
```

### Minimum Spanning Tree (MST)

\begin{itemize}
    \item Tree
\end{itemize} 

|       A graph in which any two nodes are connected by exactly one path is known as a \textit{tree}.

\begin{itemize}
    \item Spanning Tree
\end{itemize} 

|       An undirected graph whose edges are structured as a tree is defines as a \textit{Spanning Tree}.

|       \textbf{Spanning Tree of Graph G is:} G'(V',E')


$$V' = V$$

$$    E' \subset E $$


$$    E' = |V|-1$$


The graph can have more than one spanning tree.

|       Spanning tree should not be disconnected and not contain any cycle. By removing one edge from the Spanning tree will make it disconnected. By adding one edge to the Spanning tree will create a loop. A complete (Each vertices connected with each other) undirected graph can have $n^{n-2}$ number of spanning trees where n is the number of vertices. Every connected and undirected graph has at least one Spanning Tree. Disconnected graph doesn't have any spanning tree. From a complete graph by removing $max(edges-n+1)$ edges we can construct a spanning tree.

\begin{itemize}
    \item Minimum Spanning Tree
\end{itemize} 

|       A spanning tree whose total length is least of all spanning trees on a given set of points is known as a Minimum Spanning Tree (MST).

|       If each edge has distinct weights then there will be only one and unique MST.

\begin{itemize}
    \item Remark
\end{itemize} 

|       The geometric MST computed from Euclidean distances between points in a 2D Euclidean geometric graph is the restriction.

### Convex Hull

|       A collection of the boundaries of one or more simple polygons that have a subset of the points for their vertices and that collectively contain all the points, is defined as a hull of a set of points embedded in 2D Euclidean space. 

|       If a hull contains all the straight line segments connecting any pair of points in its interior, is known as a convex hull. The convex hull bounds a single polygon. After deleting the points on the convex hull, a convex hull called peeled convex hull is computed. 

### Alpha Hull

|       Most of proximity graphs (neighborhood graph) represent the nonconvex shape of a set of points on the plane. A geometric graph whose edges are determined by an indicator function based on distances between a given set of points in a metric space, is known as a proximity graph. An open disk $D$ is used to define the indicator function.

|       If a point is on the boundary of $D$ then $D$ \textit{touches} a point and if a point is in $D$ then $D$ \textit{contains} a point. An open disk of radius r is defined as $D(r)$. 

|       An alpha shape [@inproceedings33] is a collection of one or more simple polygons [@article37;@inproceedings44]. An edge exists between any pair of points that can be touched by an open disk $D(\alpha)$ containing no points, is defined as an alpha shape graph.

|       A value of $\alpha$ to be the average value of the edge lengths in the MST [@article37;@inproceedings44]. The large values like 90th percentile of the MST edge lengths are used, because to reduce noise. If the percentile exceeds a tenth, clamp the value at one-tenth the width of a frame, because it prevents in including sparse or striated point sets in a single alpha graph. 


```{r, echo=FALSE, out.width="50%", fig.align='center',fig.cap="\\label{scagimg4}Convex hull and alpha hull"}
knitr::include_graphics(here::here("leaffeatures","Figures","c5.png"))
```



### Preprocessing

|           To improve the performance of the algorithm and robustness of the measures, preprocessing techniques as binning and deleting outliers are used before computing geometric graphs.

\begin{itemize}
    \item Binning
\end{itemize} 

|       As the first step of binning, the data are normalized to the unit interval. Then use a 40 by 40 hexagonal grid to aggregate the points in each scatterplot. Reduce the bin size by half and rebin until no more than 250 non emty cells, if there are more than 250 non empty cells. By efficiency (too many bins slow down calculations of the geometric graphs) and sensitivity (too few bins obscure features in the scatterplots), the choice of bin size is constrained. 

|        To improve the performance, hexagon binning is used. To manage the problem of having to many points that start to overlap, hexagon binning is used. The plots of hexagonal binning are density rather than points. To use hexagons instead of squares for binning a 2D surface as a plane, there are many reasons. Hexagons are more similar to circle than square. 

|        To keep scagnostics orientation-independent this bias reduction is important. To attenuate the influence of binning, stabilizing transformation is used when computing scagnostics from binned data.


The weight function is defined as;

\begin{equation}
    \omega = 0.7 + \frac{0.3}{1 + t^2}
    \label{w2}
\end{equation}
 
where $t=\frac{n}{500}$. ($n$ is the number of vertex)

|        If $n>2000$ then this function is fairly constant. By using hex binning the shape and the parameters of the function is determined. In computing Sparse, Skewed and Convex scagnostics this weight function is used to adjust for bias.   

\begin{itemize}
    \item Deleting Outliers
\end{itemize} 

|       To improve robustness of the scagnostics, deleting outliers can be used. A vertex whose adjacent edges in the MST all have a weight (length) greater than $\omega$ is defined as an outlier in this context. By considering nonparametric criterion for the simplicity and Tukey's idea choose the following weight calculation.

\begin{equation}
    \omega = q_{75} + 1.5(q_{75} - q_{25})
    \label{w1}
\end{equation}

where $q_{75}$ is the 75th percentile of the MST edge lengths and $(q_{75} - q_{25})$ is the Interquartile range of the edge lengths. 



### Degree of a Vertex

|        The degree of a vertex in an undirected graph is know as the number of edges associated with the vertex.

\textbf{Eg:- Vertices of degree 2} There are 2 edges associated with each vertex.


```{r, echo=FALSE, out.width="40%", fig.align='center',fig.cap="\\label{scagimg6}Vertices of degree 2"}
knitr::include_graphics(here::here("leaffeatures","Figures","c3.png"))
```

\begin{table}[h!]
\centering
\begin{tabular}{l c}
\hline
\multicolumn{1}{c}{Geometric Graphs} & Notation \\ \hline
Convex Hull                            & H        \\ %\hline
Alpha Hull                             & A        \\ %\hline
Minimum Spanning Tree                  & T        \\ \hline
\end{tabular}
\caption{Notations of Geometric Graphs}
\label{tab:gg}
\end{table}

### Density Measures

|       Detect different distributions of scattered points in density measures.

\begin{itemize}
    \item Outlying
\end{itemize}

\begin{equation}
    C_{outlying} = \frac{length(T_{outliers})}{length(T)}
\end{equation}

|        The outlying measure calculate before deleting the outliers for the other measures. The proportion of the total edge length of the minimum spanning tree accounted for by the total length of edges adjacent to outlying points is used to calculate the outlying measure.

\begin{itemize}
    \item Skewed
\end{itemize}

\begin{equation}
    q_{skew} = \frac{q_{90}-q_{50}}{q_{90}-q_{10}}
\end{equation} 

\begin{equation}
    C_{skew} = 1-\omega(1-q_{skew})
\end{equation}

where $\omega$ is the weight function (\ref{w2}).

|        The skewed measure is the first measure of relative density which is a relatively robust measure of skewness in the distribution of edge lengths. After adaptive binning skewed tends to decrease with $n$.

\begin{itemize}
    \item Sparse
\end{itemize}

\begin{equation}
    C_{sparse} = \omega q_{90}
\end{equation} 

where $\omega$ is the weight function (\ref{w2}) and $q_{90}$ is the 90th percentile of the distribution of edge lengths in the MST.

|        The second relative density measure is Sparse measure that measures whether points in a 2D scatterplot are confined to a lattice or a small number of locations on the plane.

|        If the number of points is extremely small or tuples are produced by the product of categorical variables, then sparse can be happen.

\begin{equation}
    q_{90} = \alpha statistic
\end{equation} 

|        The $\alpha$ statistic exceeds unity (e.g., when all points fall on either of the two diagonally opposing vertices
of a square), clamp the value to 1 in the extremely rare event [@article37,@inproceedings44].


\begin{itemize}
    \item Clumpy
\end{itemize}

\begin{equation}
    C_{clumpy} = max_j[1-\frac{max_k[length(e_k)]}{length(e_j)}]
\end{equation} 

|        Clustering points are not indicated by an extreme distribution of MST edge lengths. Therefore RUNT statistic [@article37,@inproceedings44] which is another measure based on the MST, is introduced. The smaller of the number of leaves of each of the two subtrees joined at that node isdefined as the runt size of a dendogram node. There is an association between runt size ($r_j$) each edge ($e_j$) in the MST because there is an isomorphism between a single-linkage dendrogram and the MST.

|        The smaller of the two subsets of edges that are still connected to each of the two vertices in $e_j$ after deleting edges in the MST with lengths less than length($e_j$), is known as the RUNT graph ($R_j$) [@article37,@inproceedings44].

|       The RUNT-based measure responds to clusters with small maximum intracluster distance relative to the length of their nearest-neighbor inter-cluster distance [@article37,@inproceedings44]. In the formula j runs over all edges in MST and k runs over all edges in RUNT graph.

\begin{itemize}
    \item Striated
\end{itemize}

\begin{equation}
    C_{striated} = \frac{1}{|V|}\sum_{\nu \in V^{(2)}}^{}I(\cos\theta_{e(\nu,a)e(\nu,b)}<-0.75)
\end{equation} 

where $V^{(2)} \subseteq V$ and $I()$ be an indicator function.

|        Striated define the coherence in a set of points as the presence of relatively smooth paths in the minimum spanning tree. 
The measure is based on the number of adjacent edges whose cosine is less than minus 0.75.

### Shape Measures

|        Both topological and geometric aspects of shape of a set of scattered points is considered. As an example, a set of scattered points on the plane appeared to be connected, convex and so forth, want to know under the shape measures. By definition scattered points are not like this. Therefore to make inferences additional machinery (based on geometric graphs) is needed. By measuring the aspects of the convex hull, the alpha hull, and the minimum spanning tree is determined.  

\begin{itemize}
    \item Convex
\end{itemize}

\begin{equation}
    C_{convex} = \omega[area(A)/area(H)]
\end{equation}

where $\omega$ is the weight function (\ref{w2}).

|       The ratio of the area of the alpha hall(A) and the area of the convex hull(H) is the base of measuring convexity. 


```{r, echo=FALSE, out.width="70%", fig.align='center',fig.cap="\\label{scagimg21}"}
knitr::include_graphics(here::here("leaffeatures","Figures","c1.png"))
```


\begin{itemize}
    \item Skinny
\end{itemize}

\begin{equation}
    C_{skinny} = 1- \frac{\sqrt{4\pi area(A)}}{perimeter(A)}
\end{equation}


|       Roughly, the skinny is measured by using the corrected and normalized ratio of perimeter to area of a polygon measures.

```{r, echo=FALSE, out.width="50%", fig.align='center',fig.cap="\\label{scagimg31}"}
knitr::include_graphics(here::here("leaffeatures","Figures","c2.png"))
```

\begin{itemize}
    \item Stringy
\end{itemize}	    

\begin{equation}
    C_{stringy} = \frac{|V^{(2)}|}{|V| - |V^{(1)}|}
\end{equation} 

where $V$ is the number of vertices.

|       A skinny shape with no branches is known as a stringy shape. By counting the vertices of degree 2 in the minimum spanning tree and comparing them to the overall number of vertices minus the
number of single-degree vertices, skinny measure is calculated.

|       To adjust for negative skew in its conditional distribution of $n$, cube the stringy measure. 


### Association Measure

|        Symmetric and relatively robust measure of association are interested.

\begin{itemize}
    \item Monotonic
\end{itemize}

\begin{equation}
    C_{monotonic} = r^2_{Spearman}
\end{equation}

|        To assess the monotonicity in a scatter plot, the squared spearman correlation coefficient is used. This is the only coefficient not based on a subset of the Delaunay graph [@article37].

|        In calculating monotonicity, squared the coefficient because to consider the large values and to remove the distinction between positive and negative coefficients (Because assume that the investigators are more interested in strong relationships rather than negative or positive).
	
```{r, echo=FALSE, out.width="50%", fig.align='center',fig.cap="\\label{scagnostics}Exploring scatter plots by their scagnostics"}
knitr::include_graphics(here::here("leaffeatures","Figures","sca.png"))
```	
	
```{r, echo=FALSE, out.width="50%", fig.align='center',fig.cap="\\label{scp}Preprocessing for Scagnostics"}
knitr::include_graphics(here::here("leaffeatures","Figures","scp.png"))
```	



|        We measured the scagnostic features for Cartesian and Polar coordinates separately.


```{r, echo=FALSE, out.width="40%", fig.align='center',fig.cap="\\label{pc}Polar coordinate"}
knitr::include_graphics(here::here("leaffeatures","Figures","pc.png"))
```	



# Example

|       There are two color image leaf datasets (Flavia ,Swedish) are used. After passing through the required image processing steps, binary image is extracted which has a white foreground and black background.


```{r, echo=FALSE, out.width="50%", fig.align='center',fig.cap="\\label{fl5}Image processing steps of Flavia dataset"}
knitr::include_graphics(here::here("leaffeatures","Figures","fl5.png"))
```	


|       The leaf images are taken as the closest ones. Therefore to find the best contour among several contours, can use the contour which contains the center of leaf image. Identify the best contour is really important when extracting shape features.


```{r, echo=FALSE, out.width="50%", fig.align='center',fig.cap="\\label{flexe}Example feature extraction of Flavia dataset"}
knitr::include_graphics(here::here("leaffeatures","Figures","fl_example.png"))
```	








# Software


\begin{table}[!ht]
\resizebox{\textwidth}{!}{%
\begin{tabular}{ccc}
\hline
Feature name & Software & Package \\ \hline
             &          &         \\
             &          &         \\
             &          &         \\
             &          &         \\ \hline
\end{tabular}%
}
\caption{Software in feature extraction}
\label{swfe}
\end{table}

# Visualization of Leaf Images in the Feature Space

## Swedish Dataset

### PCA projection 
```{r, comment=NA, message=FALSE, warning=FALSE, echo=FALSE, fig.align='center'}
calculate_pca <- function(feature_dataset){
  pcaY_cal <- prcomp(feature_dataset, center = TRUE, scale = TRUE)
  PCAresults <- data.frame(PC1 = pcaY_cal$x[, 1], 
                           PC2 = pcaY_cal$x[, 2], 
                           PC3 = pcaY_cal$x[, 3],
                           PC4 = pcaY_cal$x[, 4],
                           PC5 = pcaY_cal$x[, 5])
  return(list(prcomp_out =pcaY_cal,pca_components = PCAresults))
}

pca_projection <- function(prcomp_out, data_to_project){
  
  PCA <- scale(data_to_project, prcomp_out$center, prcomp_out$scale) %*% prcomp_out$rotation
  pca_projected <- data.frame(PC1=PCA[,1], PC2=PCA[,2], PC3=PCA[,3]) 
  return(pca_projected)
  
}

data_new <- read.csv("data_all_with_label.csv", header = TRUE)

features <- data_new[, c(3:10,12:53)] # remove Outlying_polar and Outlying_contour

pca_ref_calc <- calculate_pca(features)
# combine features and PCs' into a one dataframe
data_new$PC1 <- pca_ref_calc$pca_components$PC1
data_new$PC2 <- pca_ref_calc$pca_components$PC2
data_new$PC3 <- pca_ref_calc$pca_components$PC3
data_new$PC4 <- pca_ref_calc$pca_components$PC4
data_new$PC5 <- pca_ref_calc$pca_components$PC5


p1 <- ggplot(data_new,aes(x=PC1,y=PC2, col=Shape_label)) + 
  geom_point() +coord_equal() + theme(legend.position = "none")

p2 <- ggplot(data_new,aes(x=PC1,y=PC3, col=Shape_label)) + 
  geom_point() +coord_equal() + theme(legend.position = "none")

p3 <- ggplot(data_new,aes(x=PC2,y=PC3, col=Shape_label)) + 
  geom_point() +coord_equal() + theme(legend.position = "none")

p4 <- ggplot(data_new,aes(x=PC1,y=PC4, col=Shape_label)) + 
  geom_point() +coord_equal() + theme(legend.position = "none")

p5 <- ggplot(data_new,aes(x=PC1,y=PC5, col=Shape_label)) + 
  geom_point() +coord_equal() + theme(legend.position = "none")

p6 <- ggplot(data_new,aes(x=PC2,y=PC4, col=Shape_label)) + 
  geom_point() +coord_equal() + theme(legend.position = "none")

p7 <- ggplot(data_new,aes(x=PC2,y=PC5, col=Shape_label)) + 
  geom_point() +coord_equal() + theme(legend.position = "none")

p8 <- ggplot(data_new,aes(x=PC3,y=PC4, col=Shape_label)) + 
  geom_point() +coord_equal() + theme(legend.position = "none")

p9 <- ggplot(data_new,aes(x=PC3,y=PC5, col=Shape_label)) + 
  geom_point() +coord_equal() + theme(legend.position = c(-0.9,-0.62) ,legend.direction = "horizontal")

p1 + p2 + p3 + p4 + p5 + p6 + p7 + p8 + p9 + plot_layout(ncol = 3, widths = c(3,3))
```


## Flavia Dataset

### PCA projection 
```{r, comment=NA, message=FALSE, warning=FALSE, echo=FALSE, fig.align='center'}

data_new <- read.csv("data_all_with_label_flavia.csv", header = TRUE)

features <- data_new[, c(3:10,12:53)] # remove Outlying_polar and Outlying_contour

pca_ref_calc <- calculate_pca(features)
# combine features and PCs' into a one dataframe
data_new$PC1 <- pca_ref_calc$pca_components$PC1
data_new$PC2 <- pca_ref_calc$pca_components$PC2
data_new$PC3 <- pca_ref_calc$pca_components$PC3
data_new$PC4 <- pca_ref_calc$pca_components$PC4
data_new$PC5 <- pca_ref_calc$pca_components$PC5


p11 <- ggplot(data_new,aes(x=PC1,y=PC2, col=Shape_label)) + 
  geom_point() +coord_equal()+ theme(legend.position = "none")

p12 <- ggplot(data_new,aes(x=PC1,y=PC3, col=Shape_label)) + 
  geom_point() +coord_equal()+ theme(legend.position = "none")

p13 <- ggplot(data_new,aes(x=PC2,y=PC3, col=Shape_label)) + 
  geom_point() +coord_equal()+ theme(legend.position = "none")

p14 <- ggplot(data_new,aes(x=PC1,y=PC4, col=Shape_label)) + 
  geom_point() +coord_equal()+ theme(legend.position = "none")

p15 <- ggplot(data_new,aes(x=PC1,y=PC5, col=Shape_label)) + 
  geom_point() +coord_equal()+ theme(legend.position = "none")

p16 <- ggplot(data_new,aes(x=PC2,y=PC4, col=Shape_label)) + 
  geom_point() +coord_equal()+ theme(legend.position = "none")

p17 <- ggplot(data_new,aes(x=PC2,y=PC5, col=Shape_label)) + 
  geom_point() +coord_equal()+ theme(legend.position = "none")

p18 <- ggplot(data_new,aes(x=PC3,y=PC4, col=Shape_label)) + 
  geom_point() +coord_equal()+ theme(legend.position = "none")

p19 <- ggplot(data_new,aes(x=PC3,y=PC5, col=Shape_label)) + 
  geom_point() +coord_equal()+ theme(legend.position = c(-0.9,-0.62) ,legend.direction = "horizontal")


p11 + p12 + p13 + p14 + p15 + p16 + p17 + p18 + p19 + plot_layout(ncol = 3, widths = c(3,3))
```


# Discussion and Conclusions

# Reference
